{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c17f023",
   "metadata": {},
   "source": [
    "Sources: \n",
    "\n",
    "https://www.kaggle.com/code/amyjang/monet-cyclegan-tutorial/notebook\n",
    "\n",
    "https://www.tensorflow.org/tutorials/load_data/tfrecord\n",
    "\n",
    "https://www.tensorflow.org/tutorials/generative/cyclegan\n",
    "\n",
    "https://www.kaggle.com/code/zahid0/gan-tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c80604d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# !pip install git+https://github.com/tensorflow/examples.git\n",
    "# from tensorflow_examples.models.pix2pix import pix2pix\n",
    "# import time\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, BatchNormalization, LeakyReLU, Conv2DTranspose, Dropout, ReLU, Input, Concatenate\n",
    "from keras.layers import ZeroPadding2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e2cea25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#importing the photos as files according to tutorial kaggle recommends to use\n",
    "#gets each file from monet and photo tfrec folder\n",
    "monet_files = tf.io.gfile.glob('/Users/sam/gan_kaggle/files/monet_tfrec/*.tfrec')\n",
    "photo_files = tf.io.gfile.glob('/Users/sam/gan_kaggle/files/photo_tfrec/*.tfrec')\n",
    "print(len(monet_files))\n",
    "print(len(photo_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48f22121",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 00:21:48.899910: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-03 00:21:48.899927: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "#setting the formats, for the parser to get into tf dataset\n",
    "formats = {\n",
    "    'image': tf.io.FixedLenFeature([], tf.string),\n",
    "    'image_name': tf.io.FixedLenFeature([], tf.string)\n",
    "}\n",
    "def tfrecord_img(instance):\n",
    "    #parsing the single instance with proper format\n",
    "    img = tf.io.parse_single_example(instance, formats)\n",
    "    img = img['image']\n",
    "    #decoding the image to show it is a rgb color scheme\n",
    "    img_decode = tf.image.decode_jpeg(img, channels = 3)\n",
    "    #casting the image pixels to float 32 and normalizing to [-1, 1]\n",
    "    img_cast = (tf.cast(img_decode, tf.float32) / 127.5) - 1\n",
    "    #reshaping to (256, 256, 3) for 256 256 size and 3 for channel\n",
    "    img_reshape = tf.reshape(img_cast, [*[256, 256], 3])\n",
    "    #randomly flipping images across the y-axis\n",
    "    img_final = tf.image.random_flip_left_right(img_reshape)\n",
    "    return img_final\n",
    "    \n",
    "    \n",
    "monet_dataset = tf.data.TFRecordDataset(monet_files).map(tfrecord_img, \n",
    "                                                         num_parallel_calls=tf.data.AUTOTUNE).batch(1)\n",
    "photo_dataset = tf.data.TFRecordDataset(photo_files).map(tfrecord_img, \n",
    "                                                         num_parallel_calls=tf.data.AUTOTUNE).batch(1)\n",
    "# monet_dataset = monet_dataset.prefetch(buffer_size=16)\n",
    "# photo_dataset = photo_dataset.prefetch(buffer_size=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a70a2ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-03 00:21:48.970249: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfRangeError\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/data_science/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:787\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 787\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_internal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n",
      "File \u001b[0;32m~/miniconda3/envs/data_science/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:770\u001b[0m, in \u001b[0;36mOwnedIterator._next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecution_mode(context\u001b[38;5;241m.\u001b[39mSYNC):\n\u001b[0;32m--> 770\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator_get_next\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    772\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_types\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    773\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_output_shapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    775\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    776\u001b[0m     \u001b[38;5;66;03m# Fast path for the case `self._structure` is not a nested structure.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/data_science/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3017\u001b[0m, in \u001b[0;36miterator_get_next\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   3016\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 3017\u001b[0m   \u001b[43m_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_from_not_ok_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3018\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_FallbackException:\n",
      "File \u001b[0;32m~/miniconda3/envs/data_science/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7214\u001b[0m e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mOutOfRangeError\u001b[0m: {{function_node __wrapped__IteratorGetNext_output_types_1_device_/job:localhost/replica:0/task:0/device:CPU:0}} End of sequence [Op:IteratorGetNext]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#printing the pictures of monet and photos\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m monet_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmonet_dataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m photo_single \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(photo_dataset))\n\u001b[1;32m      6\u001b[0m fig, (ax, ax1) \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/data_science/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:789\u001b[0m, in \u001b[0;36mOwnedIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    787\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_internal()\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOutOfRangeError:\n\u001b[0;32m--> 789\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#printing the pictures of monet and photos\n",
    "\n",
    "monet_single = next((iter(monet_dataset)))\n",
    "photo_single = next(iter(photo_dataset))\n",
    "\n",
    "fig, (ax, ax1) = plt.subplots(1, 2, figsize = (10, 10))\n",
    "ax.imshow(monet_single[0] * 0.5 + .5)\n",
    "ax.title.set_text('Monet Painting')\n",
    "ax1.imshow(photo_single[0] * 0.5 + .5)\n",
    "ax1.title.set_text('Photo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468221c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input)\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "\n",
    "    for i in range(2):\n",
    "    plt.subplot(1, 2, i+1)\n",
    "    plt.title(title[i])\n",
    "    # getting the pixel values between [0, 1] to plot it.\n",
    "    plt.imshow(display_list[i] * 0.5 + 0.5)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799bb38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.tensorflow.org/tutorials/generative/pix2pix#build_the_generator\n",
    "\n",
    "#encoder(downsampler)\n",
    "def encoder(filters, size, apply_batchnorm=True):\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters, size,\n",
    "                    strides=2, padding='same',\n",
    "                    kernel_initializer=initializer, use_bias = False))\n",
    "    if apply_batchnorm:\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(LeakyReLU())\n",
    "    return model\n",
    "    \n",
    "    \n",
    "#decoder (upsampler)\n",
    "def decoder(filters, size, apply_dropout=False):\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    model = Sequential()\n",
    "    model.add(Conv2DTranspose(filters, size, strides=2, padding='same',\n",
    "                             kernel_initializer=initializer, use_bias=False))\n",
    "    model.add(BatchNormalization())\n",
    "    if apply_dropout:\n",
    "        model.add(Dropout(0.5))\n",
    "    model.add(ReLU())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39820acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator\n",
    "def Generator():\n",
    "    inputs = Input(shape=[256, 256, 3])\n",
    "    down_stack = [\n",
    "        encoder(64, 4, apply_batchnorm=False),\n",
    "        encoder(128, 4),\n",
    "        encoder(256, 4),\n",
    "        encoder(512, 4),\n",
    "        encoder(512, 4),\n",
    "        encoder(512, 4),\n",
    "        encoder(512, 4),\n",
    "        encoder(512, 4)\n",
    "    ]\n",
    "    up_stack = [\n",
    "        decoder(512, 4, apply_dropout=True),\n",
    "        decoder(512, 4, apply_dropout=True),\n",
    "        decoder(512, 4, apply_dropout=True),\n",
    "        decoder(512, 4),\n",
    "        decoder(256, 4),\n",
    "        decoder(128, 4),\n",
    "        decoder(64, 4)\n",
    "    ]\n",
    "    initializer = tf.random_normal_initializer(0, 0.02)\n",
    "    last = Conv2DTranspose(3, 4, strides = 2, padding = 'same', kernel_initializer=initializer,\n",
    "                          activation = 'tanh')\n",
    "    x = inputs\n",
    "    skips = []\n",
    "    for down in down_stack:\n",
    "        x = down(x)\n",
    "        skips.append(x)\n",
    "    skips = reversed(skips[:-1])\n",
    "    \n",
    "    for up, skip in zip(up_stack, skips):\n",
    "        x = up(x)\n",
    "        x = Concatenate()([x, skip])\n",
    "    x = last(x)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a3a15d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = Generator()\n",
    "tf.keras.utils.plot_model(generator, show_shapes=True, dpi=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6938385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting loss function to be binary crossentropy\n",
    "binary_loss = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
    "#building loss function for our discriminator\n",
    "def discriminator_loss(real_image, generated_image):\n",
    "    loss_for_real = binary_loss(tf.ones_like(real_image), real_image)\n",
    "    loss_for_generated = binary_loss(tf.zeros_like(generated_image), generated_image)\n",
    "    total_loss_for_discriminator = loss_for_generated + loss_for_real\n",
    "    return total_loss_for_discriminator * 0.5\n",
    "#building loss function for generator\n",
    "def generator_loss(generated_image):\n",
    "    return binary_loss(tf.ones_like(generated_image), generated_image)\n",
    "\n",
    "#utilizing cycle consistency, put this in write up\n",
    "def cycle_loss(real_image, cycled_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
    "    return 10 * loss\n",
    "\n",
    "#utilizing identity loss, put in write up\n",
    "def identity_loss(real_image, same_real_image):\n",
    "    loss = tf.reduce_mean(tf.abs(real_image - same_real_image))\n",
    "    return 10 * loss * 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208946ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ = 100\n",
    "\n",
    "def generator_loss(disc_generated_output, gen_output, target):\n",
    "    gan_loss = binary_loss(tf.ones_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    # Mean absolute error\n",
    "    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n",
    "\n",
    "    total_gen_loss = gan_loss + (lambda_ * l1_loss)\n",
    "\n",
    "    return total_gen_loss, gan_loss, l1_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983002fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#discriminator\n",
    "def Discriminator():\n",
    "    initializer = tf.random_normal_initializer(0., 0.02)\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=[256, 256, 3], name='input_image')\n",
    "\n",
    "    x = inputs\n",
    "\n",
    "    down1 = encoder(64, 4, False)(x)  \n",
    "    down2 = encoder(128, 4)(down1)  \n",
    "    down3 = encoder(256, 4)(down2) \n",
    "    zero_pad1 = ZeroPadding2D()(down3) \n",
    "    conv = Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1) \n",
    "\n",
    "    batchnorm1 = BatchNormalization()(conv)\n",
    "\n",
    "    leaky_relu = LeakyReLU()(batchnorm1)\n",
    "\n",
    "    zero_pad2 = ZeroPadding2D()(leaky_relu)  # (batch_size, 33, 33, 512)\n",
    "\n",
    "    last = Conv2D(1, 4, strides=1,\n",
    "                                kernel_initializer=initializer)(zero_pad2)  # (batch_size, 30, 30, 1)\n",
    "\n",
    "    return tf.keras.Model(inputs=inputs, outputs=last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bfdf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = Discriminator()\n",
    "tf.keras.utils.plot_model(discriminator, show_shapes=True, dpi=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27002284",
   "metadata": {},
   "outputs": [],
   "source": [
    "##discriminator loss\n",
    "def discriminator_loss(disc_real_output, disc_generated_output):\n",
    "    real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n",
    "\n",
    "    generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n",
    "\n",
    "    total_disc_loss = real_loss + generated_loss\n",
    "\n",
    "    return total_disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb9318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting generator and discriminator for monet and photo\n",
    "generate_monet = Generator()\n",
    "generate_photo = Generator()\n",
    "discriminate_monet = Discriminator()\n",
    "discriminate_photo = Discriminator()\n",
    "\n",
    "#using adam as optimizer with learning rate 0.0002 and beta of 0.5\n",
    "adam = tf.keras.optimizers.Adam(0.0002, beta_1=0.5)\n",
    "#setting optimizers\n",
    "generate_monet_opt = adam\n",
    "generate_photo_opt = adam\n",
    "discriminate_monet_opt = adam\n",
    "discriminate_photo_opt = adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3474a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_images(model, test_input):\n",
    "    prediction = model(test_input, training=True)\n",
    "    display_list = [test_input[0], prediction[0]]\n",
    "    title = ['Input Image', 'Predicted Image']\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,10))\n",
    "    ax1.imshow(display_list[0] * 0.5 + 0.5)\n",
    "    ax2.imshow(display_list[1] * 0.5 + 0.5)\n",
    "    ax1.title.set_text(title[0])\n",
    "    ax2.title.set_text(title[1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27bc295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate monet from photo\n",
    "for example_input in photo_dataset.take(1):\n",
    "    generate_images(generate_monet, example_input)\n",
    "for example_input in monet_dataset.take(1):\n",
    "    generate_images(generate_photo, example_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
